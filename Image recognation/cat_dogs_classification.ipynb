{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ESuXwdOKTmEe"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d salader/dogs-vs-cats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmaYFvl2UVbd",
        "outputId": "dd9d21b5-a241-429a-e786-32d2691e3276"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading dogs-vs-cats.zip to /content\n",
            "100% 1.06G/1.06G [00:10<00:00, 91.2MB/s]\n",
            "100% 1.06G/1.06G [00:10<00:00, 106MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile \n",
        "zippedfile = zipfile.ZipFile('/content/dogs-vs-cats.zip', 'r')\n",
        "zippedfile.extractall('/content')\n",
        "zippedfile.close()"
      ],
      "metadata": {
        "id": "SURkjxSVU7-A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D , Flatten , Dense, Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "S1VRFSBBVp7R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\"\"\" now we add a convolution to our first layer to extract features from the pics that we add the so we need to use Conv2D funcion with a specific number\n",
        "of filters (karnels) this filters are slide into the pic and extarct features (increse number of filter = increase the ability of extract complex features\n",
        "from pics and also may conduct to an overfitting , after this each filter generat a matrix (3x3) in our case to be used in weights channels and help neural \n",
        "network be more precisable like the bias value \"\"\"\n",
        "model.add(Conv2D(32,(3,3),input_shape=(256,256,3),activation=\"relu\")) #inpute_shpe --> 64x64 dimension --> 3 vlues represent the color used (principal colors red , green , blue ) in grey scale there is only 1 value\n",
        "model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2))) #use it to reduce the number of feature map in this layer \n",
        "model.add(Flatten())# reshape the tensor into a one-dimentionel (after it was 3d) so now we need a hidden layer to make the output of this into it \n",
        "model.add(Dense(units=128 , activation=\"relu\"))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(units=1 , activation='sigmoid'))#here w use a segmoid function bcs the prediction will be 1 or 0 (cat or dog )"
      ],
      "metadata": {
        "id": "tbgWR694XcnJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" optimizer how reverse propagation adam is the most used and work good with large data ,  for the loss and metrics visit keras documentation\"\"\"\n",
        "model.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics =['accuracy']) "
      ],
      "metadata": {
        "id": "Ep0MVG0hX7uA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_gen = ImageDataGenerator(rescale=1./255 , shear_range=0.2,zoom_range=0.2,horizontal_flip=True) #bcs we have 255 color \n",
        "train_set = train_gen.flow_from_directory('/content/train',shuffle=True,target_size = (256,256),batch_size=32 , class_mode =\"binary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aka4w4s8X_Se",
        "outputId": "a90d1d09-e779-4e00-ecbe-d472b3aa2e5b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = ImageDataGenerator(rescale=1./255 , shear_range=0.2,zoom_range=0.2,horizontal_flip=True) #bcs we have 255 color \n",
        "test_set = train_gen.flow_from_directory('/content/test',shuffle=True,target_size=(256,256),batch_size=32 , class_mode =\"binary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5msX4IATYUyv",
        "outputId": "3c1bc774-3f1c-4bb7-894c-26aaf6b6d97a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(train_set,steps_per_epoch=500,epochs=5,validation_data=test_set,validation_steps=155)\n",
        "\"\"\" about the number that we choose in steps per epochs , epochs , validaition_steps ... etc so \n",
        " in the first we choose a batch size in our case we have small data so after process 32 data examples the model update \n",
        " his weights that why batch_size=32 , for the steps per epoch mark that in evry epoch we will process the entire dataset\n",
        " so 332*32=10624= len(dataset) that means that we are updating our weights 332 time in each epoch \"\"\""
      ],
      "metadata": {
        "id": "3_t9n7KbYh0P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "a7510aae-d297-4a6c-d43d-1687787bd052"
      },
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-767a5eb3ac33>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_set,steps_per_epoch=500,epochs=5,validation_data=test_set,validation_steps=155)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "500/500 [==============================] - 4334s 9s/step - loss: 0.8641 - accuracy: 0.5774 - val_loss: 0.6461 - val_accuracy: 0.6169\n",
            "Epoch 2/5\n",
            "500/500 [==============================] - 4296s 9s/step - loss: 0.6578 - accuracy: 0.6056 - val_loss: 0.6688 - val_accuracy: 0.6121\n",
            "Epoch 3/5\n",
            "500/500 [==============================] - 4212s 8s/step - loss: 0.6329 - accuracy: 0.6478 - val_loss: 0.6039 - val_accuracy: 0.6728\n",
            "Epoch 4/5\n",
            "500/500 [==============================] - 4203s 8s/step - loss: 0.5983 - accuracy: 0.6834 - val_loss: 0.5656 - val_accuracy: 0.7133\n",
            "Epoch 5/5\n",
            "500/500 [==============================] - 4216s 8s/step - loss: 0.5716 - accuracy: 0.7061 - val_loss: 0.5399 - val_accuracy: 0.7278\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' about the number that we choose in steps per epochs , epochs , validaition_steps ... etc so \\n in the first we choose a batch size in our case we have small data so after process 32 data examples the model update \\n his weights that why batch_size=32 , for the steps per epoch mark that in evry epoch we will process the entire dataset\\n so 332*32=10624= len(dataset) that means that we are updating our weights 332 time in each epoch '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import keras.utils as image\n",
        "test_image = image.load_img('/content/121594.jpg',target_size=(256,256))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image,axis=0)\n",
        "result = model.predict(test_image)\n",
        "print(train_set.class_indices)\n",
        "if result[0][0]==1 : #the model can made wrong prediction with 30%-50% just e need t traine more  \n",
        "  print(\"dog\")\n",
        "else :\n",
        "  print(\"cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2KuvdB3QX5W",
        "outputId": "4ace029c-f5dc-45b0-dec8-d80a808e6c46"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 108ms/step\n",
            "{'cats': 0, 'dogs': 1}\n",
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_98uUJ3R_Kf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}